{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72edec8c",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 3: Fine-Tuning Pretrained Transformer on Text Classification Task\n",
    "\n",
    "In this lab, you will apply the concepts learned by fine-tuning a pre-trained Transformer model on a text classification task using the Hugging Face `transformers` library.\n",
    "\n",
    "## Objectives:\n",
    "- Learn to load a pre-trained model from Hugging Face.\n",
    "- Fine-tune the model on a text classification dataset.\n",
    "- Evaluate and save the fine-tuned model.\n",
    "\n",
    "### Instructions:\n",
    "Follow the steps below to complete the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16efac3",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries\n",
    "\n",
    "In this lab, we will use the following Python libraries:\n",
    "\n",
    "- **Transformers**: Hugging Face's `transformers` library provides pre-trained models for various Natural Language Processing (NLP) tasks. We will use this to load and fine-tune a pre-trained transformer model.\n",
    "  \n",
    "- **Datasets**: Hugging Face's `datasets` library allows easy access to a wide range of datasets and provides tools for efficient data processing.\n",
    "  \n",
    "- **Scikit-learn**: A widely-used library for machine learning, which includes tools for metrics, evaluation, and preprocessing. In this lab, we'll use it to calculate evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
    "  \n",
    "- **Accelerate**: This library allows seamless usage of different hardware (CPU, GPU) for training, helping to speed up the training process by utilizing all available resources efficiently.\n",
    "\n",
    "You can install these libraries using the following command:\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets scikit-learn accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ddeba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: datasets in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: accelerate in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: psutil in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from accelerate) (2.4.1+cu124)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install necessary libraries\n",
    "!pip install transformers datasets scikit-learn accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bec9db",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dace2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88854d17",
   "metadata": {},
   "source": [
    "## Step 2: Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9318c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:01<00:00, 16343.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing tokenizer from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21caf",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00486d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import pre-trained model and Trainer utilities\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'].shuffle().select(range(1000)),\n",
    "    eval_dataset=tokenized_datasets['test'].shuffle().select(range(1000)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f91d",
   "metadata": {},
   "source": [
    "## Step 4: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d43839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|â–ˆ         | 130/1250 [00:05<02:03,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3791567385196686, 'eval_runtime': 1.0562, 'eval_samples_per_second': 946.791, 'eval_steps_per_second': 118.349, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 253/1250 [00:11<02:17,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.532483696937561, 'eval_runtime': 1.037, 'eval_samples_per_second': 964.355, 'eval_steps_per_second': 120.544, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 379/1250 [00:16<01:34,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7745172381401062, 'eval_runtime': 1.0525, 'eval_samples_per_second': 950.128, 'eval_steps_per_second': 118.766, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 500/1250 [00:21<00:26, 28.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.266, 'grad_norm': 0.022533729672431946, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 505/1250 [00:23<02:24,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9026833772659302, 'eval_runtime': 1.0554, 'eval_samples_per_second': 947.469, 'eval_steps_per_second': 118.434, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 628/1250 [00:28<01:27,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9903187155723572, 'eval_runtime': 1.0541, 'eval_samples_per_second': 948.69, 'eval_steps_per_second': 118.586, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 754/1250 [00:34<00:53,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.055458426475525, 'eval_runtime': 1.0417, 'eval_samples_per_second': 959.996, 'eval_steps_per_second': 119.999, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 880/1250 [00:39<00:40,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0952606201171875, 'eval_runtime': 1.0462, 'eval_samples_per_second': 955.809, 'eval_steps_per_second': 119.476, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1000/1250 [00:44<00:08, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0148, 'grad_norm': 0.007087570149451494, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1003/1250 [00:46<01:03,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1020959615707397, 'eval_runtime': 1.0447, 'eval_samples_per_second': 957.207, 'eval_steps_per_second': 119.651, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1129/1250 [00:51<00:12,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1080827713012695, 'eval_runtime': 1.0256, 'eval_samples_per_second': 975.043, 'eval_steps_per_second': 121.88, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:58<00:00, 21.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1191223859786987, 'eval_runtime': 1.0287, 'eval_samples_per_second': 972.099, 'eval_steps_per_second': 121.512, 'epoch': 10.0}\n",
      "{'train_runtime': 58.3887, 'train_samples_per_second': 171.266, 'train_steps_per_second': 21.408, 'train_loss': 0.11383090591430664, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.11383090591430664, metrics={'train_runtime': 58.3887, 'train_samples_per_second': 171.266, 'train_steps_per_second': 21.408, 'total_flos': 657777638400000.0, 'train_loss': 0.11383090591430664, 'epoch': 10.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c5581",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bfc2e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "we will evaluate the performance of the fine-tuned model using the following metrics:\n",
    "\n",
    "- **Accuracy**: This measures the percentage of correctly predicted labels out of all predictions. It is useful for understanding the overall correctness of the model. \n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "    $$\n",
    "\n",
    "- **Precision**: Precision measures how many of the positive predictions made by the model are actually correct. It is especially important when the cost of false positives is high (i.e., incorrectly labeling negative examples as positive).\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "    $$\n",
    "\n",
    "- **Recall**: Recall (also known as sensitivity or true positive rate) measures how many actual positive cases the model is able to correctly identify. It is important when the cost of missing positive examples (false negatives) is high.\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "    $$\n",
    "\n",
    "- **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a balance between the two. It is particularly useful when you need to balance both false positives and false negatives.\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795b21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 121.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1191223859786987, 'eval_accuracy': 0.836, 'eval_f1': 0.836, 'eval_precision': 0.8228346456692913, 'eval_recall': 0.8495934959349594, 'eval_runtime': 1.0377, 'eval_samples_per_second': 963.644, 'eval_steps_per_second': 120.455, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# Update trainer to include custom metrics\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684fc02",
   "metadata": {},
   "source": [
    "## Step 6: Saving the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f62810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my-fine-tuned-bert/tokenizer_config.json',\n",
       " 'my-fine-tuned-bert/special_tokens_map.json',\n",
       " 'my-fine-tuned-bert/vocab.txt',\n",
       " 'my-fine-tuned-bert/added_tokens.json',\n",
       " 'my-fine-tuned-bert/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model('my-fine-tuned-bert')\n",
    "tokenizer.save_pretrained('my-fine-tuned-bert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c54418",
   "metadata": {},
   "source": [
    "## Step 7: Loading and Testing the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed79bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.9996389150619507}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load saved model and tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained('my-fine-tuned-bert')\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('my-fine-tuned-bert')\n",
    "\n",
    "# Create a classification pipeline\n",
    "classifier = TextClassificationPipeline(model=new_model, tokenizer=new_tokenizer)\n",
    "\n",
    "# Add label mapping for sentiment analysis (assuming LABEL_0 = 'negative' and LABEL_1 = 'positive')\n",
    "label_mapping = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "# Test the model\n",
    "result = classifier(\"This movie was excellent.\")\n",
    "\n",
    "# Map the result to more meaningful labels\n",
    "mapped_result = {'label': label_mapping[int(result[0]['label'].split('_')[1])], 'score': result[0]['score']}\n",
    "print(mapped_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7c1fe",
   "metadata": {},
   "source": [
    "## Exercise: Add a gradle UI\n",
    "\n",
    "pretty easy, nothing crazy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdbe529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakuk/code/ai_insikoorit/project/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy-fine-tuned-bert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m new_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy-fine-tuned-bert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a classification pipeline\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained('my-fine-tuned-bert')\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('my-fine-tuned-bert')\n",
    "# Create a classification pipeline\n",
    "classifier = TextClassificationPipeline(model=new_model, tokenizer=new_tokenizer)\n",
    "\n",
    "def evaluate_rating(prompt):\n",
    "    label_mapping = { 0: 'negative', 1: 'positive' }\n",
    "    result = classifier(prompt)\n",
    "    mapped_result = {'label': label_mapping[int(result[0]['label'].split('_')[1])], 'score': result[0]['score']}\n",
    "    return mapped_result['score'], mapped_result['label']\n",
    "\n",
    "\n",
    "def generate_interface(prompt):\n",
    "    return evaluate_rating(prompt)\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=generate_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter your code here\",\n",
    "                   label=\"Prompt\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Number(label=\"Score\"),\n",
    "        gr.Textbox(label=\"Positive or Negative?\"),\n",
    "    ],\n",
    "    title=\"Pretrained transformer with imdb dataset\",\n",
    "    description=\"This is the description\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
